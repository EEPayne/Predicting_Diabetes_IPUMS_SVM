---
title: "Analysis of Predicting Diabetes with SVMs"
author: "Elling Payne"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(fig.align = "center");
knitr::opts_chunk$set(seed = 112358);
library(tidyverse);
library(e1071);
```

```{r}
# read in preprocessed data
load("output/preprocessing/data/selected_response_and_features.RData");
```

```{r}
# without a clear idea of what kernel might work best, lest just try all kernels
# on the response vs the six predictors, Tuning Params based on CV balanced error score

# plain accuracy will not be a good evaluation metric given the imbalance in the response
# returns a list containing class errors and balanced error
# this can be passed to e1071::tune() as the error.fun parameter
balanced_error <- function(actual, predicted) {
  cm <- table(predicted, actual);
  classes <- dimnames(cm)[[1]];
  errs <- numeric(nrow(cm));
  for (i in 1:nrow(cm)) {
    TP <- cm[i, i];
    TN <- sum(diag(cm)) - TP;
    errs[i] <- 1 - ((TP + TN) / sum(cm));
  }
  return(mean(errs));
};
```

```{r}
# create training and testing set
set.seed(112358);
test.prop <- 0.2;
test.indices <- sample(1:nrow(diabetes_ipums), size=test.prop*nrow(diabetes_ipums));
test.ipums <- diabetes_ipums[test.indices,];
train.ipums <- diabetes_ipums[-test.indices,];
```


```{r}
# trying a linear kernel
set.seed(112358);
linear.tune <- tune(svm, DIABETICEV ~ ., data=train.ipums, kernel="linear", scale=TRUE,
                    cross=5, class.weights = "inverse",
                    ranges=list(cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10)),
                    error.fun=balanced_error);
best_cost <- linear.tune$best.parameters[1,1];
linear.chosen_model <- linear.tune$best.model;
```
```{r}
write.csv(as.data.frame(linear.tune$performances),
          file="output/analysis/data/tuning_results_linear.csv",
          row.names = FALSE);
plt <- ggplot(data = as.data.frame(linear.tune$performances),
              mapping = aes(x=cost, y=error)) +
  geom_line(color="skyblue") +
  labs(title = "Cost vs Balanced CV Error for Linear Kernel",
       x = "Cost", y = "Balanced Error");# +
  #annotate(geom="segment", x=best_cost, xend=best_cost, y=-Inf, yend=Inf,
      #     linetype="dashed", color="darkred");
print(plt);
ggsave("output/analysis/plots/tuning_params_vs_error_linear.png", plot=plt,
       width=8, height=6, dpi="print");
```

```{r}
# trying a polynomial kernel
set.seed(112358);
poly.tune <- tune(svm, DIABETICEV ~ ., data=train.ipums, kernel="polynomial", scale=TRUE,
                  cross=5, class.weights="inverse",
                  ranges=list(cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10),
                              degree=c(2, 3, 4)),
                  error.fun=balanced_error);
best_cost.poly <- poly.tune$best.parameters[1,1];
best_degree <- poly.tune$best.parameters[1,2];
poly.chosen_model <- poly.tune$best.model;
```
```{r}
write.csv(as.data.frame(poly.tune$performances),
          file="output/analysis/data/tuning_results_polynomial.csv",
          row.names = FALSE);
plt <- ggplot(data = as.data.frame(poly.tune$performances),
              mapping = aes(x=cost, y=error, color=as.factor(degree),
                            linetype=as.factor(degree))) +
  geom_line() +
  labs(title = "Cost and Degree vs Balanced CV Error for Polynomial Kernel",
       x = "Cost", y = "Balanced Error", color="Degree", linetype="Degree");# +
  #annotate(geom="segment", x=best_cost, xend=best_cost, y=-Inf, yend=Inf,
      #     linetype="dashed", color="darkred");
print(plt);
ggsave("output/analysis/plots/tuning_params_vs_error_polynomial.png", 
       plot = plt, width = 8, height = 6, dpi = "print");
```

```{r}
# trying a radial kernel
set.seed(112358);
radial.tune <- tune(svm, DIABETICEV ~ ., data=train.ipums, kernel="radial", scale=TRUE,
                  cross=5, class.weights="inverse",
                  ranges=list(cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10),
                              gamma=c(0.001, 0.01, 0.1, 1, 10, 100)),
                  error.fun=balanced_error);
best_cost.radial <- radial.tune$best.parameters[1,1];
best_gamma <- radial.tune$best.parameters[1,2];
radial.chosen_model <- radial.tune$best.model;
```
```{r}
write.csv(as.data.frame(radial.tune$performances),
          file="output/analysis/data/tuning_results_radial.csv",
          row.names = FALSE);
plt <- ggplot(data = as.data.frame(radial.tune$performances),
              mapping = aes(x=cost, y=error, color=as.factor(gamma),
                            linetype=as.factor(gamma))) +
  geom_line() +
  labs(title = "Cost and Gamma vs Balanced CV Error for Radial Kernel",
       x = "Cost", y = "Balanced Error", color="Gamma", linetype="Gamma");# +
  #annotate(geom="segment", x=best_cost, xend=best_cost, y=-Inf, yend=Inf,
      #     linetype="dashed", color="darkred");
print(plt);
ggsave("output/analysis/plots/tuning_params_vs_error_radial.png", plot=plt,
       width = 8, height = 6, dpi = "print");
```


```{r}
# comparison of training balanced error, training cv balanced error,
# and test balanced error between the models
traincv.linear <- min(linear.tune$performances$error);
traincv.poly <- min(poly.tune$performances$error);
traincv.radial <- min(radial.tune$performances$error);

trainpred.linear <- predict(linear.chosen_model);
trainpred.poly <- predict(poly.chosen_model);
trainpred.radial <- predict(radial.chosen_model);
trainerr.linear <- balanced_error(train.ipums$DIABETICEV, trainpred.linear);
trainerr.poly <- balanced_error(train.ipums$DIABETICEV, trainpred.poly);
trainerr.radial <- balanced_error(train.ipums$DIABETICEV, trainpred.radial);

testpred.linear <- predict(linear.chosen_model, newdata=test.ipums);
testpred.poly <- predict(poly.chosen_model, newdata=test.ipums);
testpred.radial <- predict(radial.chosen_model, newdata=test.ipums);
testerr.linear <- balanced_error(test.ipums$DIABETICEV, testpred.linear);
testerr.poly <- balanced_error(test.ipums$DIABETICEV, testpred.poly);
testerr.radial <- balanced_error(test.ipums$DIABETICEV, testpred.radial);

kernel_types <- c("linear", "polynomial", "radial");
training_cvs <- c(traincv.linear, traincv.poly, traincv.radial);
train_errs <- c(trainerr.linear, trainerr.poly, trainerr.radial);
test_errs <- c(testerr.linear, testerr.poly, testerr.radial);
chosen_cost <- c(best_cost, best_cost.poly, best_cost.radial);
chosen_degree <- c(NA, best_degree, NA);
chosen_gamma <- c(NA, NA, best_gamma);
error_df <- data.frame(kernel = kernel_types, cost = chosen_cost, 
                       degree = chosen_degree, gamma = chosen_gamma,
                       training_cv_error = training_cvs,
                       training_error = train_errs, testing_error = test_errs);
write.csv(error_df, file="output/analysis/data/tuned_model_kernel_comparison_errs.csv",
          row.names = FALSE);
plot_data <- pivot_longer(error_df, cols=c("training_cv_error", "training_error",
                                           "testing_error"),
                          names_to = "Error Type", values_to = "Error");
plt <- ggplot(data = plot_data, mapping = aes(x=kernel, y = Error,
                                              color = .data[["Error Type"]],
                                              shape = .data[["Error Type"]],
                                              fill = .data[["Error Type"]])) +
  geom_point() +
  scale_color_manual(values=c("training_cv_error"="skyblue",
                              "training_error"="coral",
                              "testing_error"="limegreen"),
                     labels=c("Train CV", "Train", "Test")) +
  scale_shape_manual(values=c("training_cv_error"=21,
                              "training_error"=22,
                              "testing_error"=23),
                     labels=c("Train CV", "Train", "Test")) +
  scale_fill_manual(values=c("training_cv_error"="skyblue",
                              "training_error"="coral",
                              "testing_error"="limegreen"),
                     labels=c("Train CV", "Train", "Test")) +
  labs(title = "Comparison of Tuned Model Error for Various Kernels",
       x = "Kernel", y = "Balanced Error", color = "Error Type",
       shape = "Error Type");
print(plt);
ggsave("output/analysis/plots/tuned_model_error_comparison.png", plot=plt,
       width = 8, height = 6, dpi = "print");
```
Based on balanced error for the test set, the polynomial kernel performs the best on this data set. Training and training CV error were also lower for the polynomial kernel. The polynomial kernel had degree 4, and the best model with this kernel in terms of training CV score had a cost value of 0.1.



```{r}
plot(poly.chosen_model, train.ipums, AGE ~ BMICALC);
```
```{r}
plot(poly.chosen_model, train.ipums, AGE ~ HOURSWRK);
```
```{r}
plot(poly.chosen_model, train.ipums, AGE ~ ALCDAYSYR);
```
```{r}
plot(poly.chosen_model, train.ipums, AGE ~ SODAPNO);
```
```{r}
plot(poly.chosen_model, train.ipums, AGE ~ FRIESPNO);
```

```{r}
plot(poly.chosen_model, train.ipums, FRIESPNO ~ SODAPNO);
```






